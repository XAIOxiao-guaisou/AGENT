"""
Antigravity æ™ºèƒ½ä¸Šä¸‹æ–‡ç®¡ç†å™¨
Intelligent Context Manager

åŠŸèƒ½ / Features:
- Token ç²¾å‡†è®¡æ•° / Precise token counting with tiktoken
- æ™ºèƒ½ä¸Šä¸‹æ–‡ä¼˜åŒ– / Intelligent context optimization
- éª¨æ¶åŒ–æˆªæ–­ç­–ç•¥ / Skeleton extraction for truncation
- è¾“å‡º Token ä¼°ç®— / Output token estimation
"""

import tiktoken
import re
import ast
from typing import Dict, List, Optional, Tuple


class ContextManager:
    """
    æ™ºèƒ½ä¸Šä¸‹æ–‡ç®¡ç†å™¨
    Intelligent Context Manager
    
    åœ¨ Token é™åˆ¶å†…æœ€å¤§åŒ–æœ‰æ•ˆä¿¡æ¯å¯†åº¦
    Maximize information density within token limits
    """
    
    def __init__(self, model: str = "gpt-4", max_tokens: int = 16384):
        """
        åˆå§‹åŒ–ä¸Šä¸‹æ–‡ç®¡ç†å™¨
        
        Args:
            model: æ¨¡å‹åç§° (ç”¨äºé€‰æ‹©æ­£ç¡®çš„ tokenizer)
            max_tokens: æœ€å¤§ Token é™åˆ¶
        """
        self.model = model
        self.max_tokens = max_tokens
        
        # åˆå§‹åŒ– tokenizer
        try:
            # DeepSeek ä½¿ç”¨ cl100k_base (ä¸ GPT-4 ç›¸åŒ)
            self.encoding = tiktoken.get_encoding("cl100k_base")
            print(f"âœ… Tokenizer initialized: cl100k_base")
        except Exception as e:
            print(f"âš ï¸ Failed to load tokenizer: {e}")
            self.encoding = None
    
    def count_tokens(self, text: str) -> int:
        """
        è®¡ç®—æ–‡æœ¬çš„ Token æ•°é‡
        Count tokens in text
        
        Args:
            text: è¦è®¡ç®—çš„æ–‡æœ¬
        
        Returns:
            Token æ•°é‡
        """
        if self.encoding is None:
            # é™çº§: ç²—ç•¥ä¼°ç®— (1 token â‰ˆ 4 chars)
            return len(text) // 4
        
        try:
            return len(self.encoding.encode(text))
        except Exception as e:
            print(f"âš ï¸ Token counting failed: {e}")
            return len(text) // 4
    
    def optimize_context(
        self, 
        files: Dict[str, str],  # {file_path: content}
        priority_files: Optional[List[str]] = None,
        reserve_tokens: int = 4096  # ä¸ºè¾“å‡ºé¢„ç•™çš„ tokens
    ) -> Dict[str, str]:
        """
        ä¼˜åŒ–ä¸Šä¸‹æ–‡,ç¡®ä¿ä¸è¶…è¿‡ Token é™åˆ¶
        Optimize context to fit within token limit
        
        ç­–ç•¥ / Strategy:
        1. ä¼˜å…ˆä¿ç•™ priority_files (å®Œæ•´å†…å®¹)
        2. å…¶ä»–æ–‡ä»¶æŒ‰ä¾èµ–å…³ç³»æ’åº
        3. å¦‚æœè¶…é™,æˆªæ–­æˆ–éª¨æ¶åŒ–ä½ä¼˜å…ˆçº§æ–‡ä»¶
        
        Args:
            files: æ‰€æœ‰å€™é€‰æ–‡ä»¶åŠå…¶å†…å®¹
            priority_files: é«˜ä¼˜å…ˆçº§æ–‡ä»¶åˆ—è¡¨ (ç›®æ ‡æ–‡ä»¶)
            reserve_tokens: ä¸º LLM è¾“å‡ºé¢„ç•™çš„ tokens
        
        Returns:
            ä¼˜åŒ–åçš„æ–‡ä»¶å­—å…¸
        """
        available_tokens = self.max_tokens - reserve_tokens
        
        print(f"ğŸ“Š Optimizing context: {len(files)} files, {available_tokens} tokens available")
        
        # åˆ†ç±»æ–‡ä»¶
        priority_set = set(priority_files or [])
        priority_content = {}
        normal_content = {}
        
        for file, content in files.items():
            if file in priority_set:
                priority_content[file] = content
            else:
                normal_content[file] = content
        
        # è®¡ç®—ä¼˜å…ˆçº§æ–‡ä»¶çš„ tokens
        priority_tokens = 0
        for file, content in priority_content.items():
            file_text = self._format_file(file, content)
            priority_tokens += self.count_tokens(file_text)
        
        print(f"ğŸ“Š Priority files: {len(priority_content)} files, {priority_tokens} tokens")
        
        result = priority_content.copy()
        remaining_tokens = available_tokens - priority_tokens
        
        if remaining_tokens < 0:
            print(f"âš ï¸ Priority files exceed token limit! Truncating...")
            # å³ä½¿ä¼˜å…ˆçº§æ–‡ä»¶ä¹Ÿéœ€è¦æˆªæ–­
            return self._truncate_priority_files(priority_content, available_tokens)
        
        # æ·»åŠ æ™®é€šæ–‡ä»¶,ç›´åˆ°è¾¾åˆ°é™åˆ¶
        for file, content in sorted(normal_content.items()):
            file_text = self._format_file(file, content)
            file_tokens = self.count_tokens(file_text)
            
            if file_tokens <= remaining_tokens:
                # å®Œæ•´ä¿ç•™
                result[file] = content
                remaining_tokens -= file_tokens
                print(f"  âœ… {file}: {file_tokens} tokens (full)")
            else:
                # å°è¯•éª¨æ¶åŒ–
                skeleton = self._skeletonize(content)
                skeleton_text = self._format_file(file, skeleton)
                skeleton_tokens = self.count_tokens(skeleton_text)
                
                if skeleton_tokens <= remaining_tokens:
                    result[file] = skeleton
                    remaining_tokens -= skeleton_tokens
                    print(f"  ğŸ“‹ {file}: {skeleton_tokens} tokens (skeleton)")
                else:
                    print(f"  â­ï¸ {file}: skipped (not enough tokens)")
                break
        
        total_tokens = available_tokens - remaining_tokens
        print(f"ğŸ“Š Context optimized: {len(result)}/{len(files)} files, {total_tokens}/{available_tokens} tokens ({total_tokens*100//available_tokens}%)")
        
        return result
    
    def _format_file(self, file_path: str, content: str) -> str:
        """æ ¼å¼åŒ–æ–‡ä»¶ä¸ºä¸Šä¸‹æ–‡å­—ç¬¦ä¸²"""
        return f"FILE: {file_path}\n```python\n{content}\n```\n"
    
    def _skeletonize(self, content: str) -> str:
        """
        æ™ºèƒ½"éª¨æ¶åŒ–"ç®—æ³•
        Intelligent skeleton extraction
        
        ç­–ç•¥ / Strategy:
        1. ä¿ç•™æ‰€æœ‰ import è¯­å¥
        2. ä¿ç•™æ‰€æœ‰ç±»å®šä¹‰å’Œå‡½æ•°ç­¾å
        3. æŠ˜å å‡½æ•°ä½“ä¸º "# ... [Implementation]"
        4. ä¿ç•™é‡è¦çš„æ³¨é‡Šå’Œæ–‡æ¡£å­—ç¬¦ä¸²
        
        Args:
            content: åŸå§‹æ–‡ä»¶å†…å®¹
        
        Returns:
            éª¨æ¶åŒ–åçš„å†…å®¹
        """
        try:
            # å°è¯•ä½¿ç”¨ AST è§£æ
            return self._skeletonize_ast(content)
        except SyntaxError:
            # é™çº§åˆ°æ­£åˆ™è¡¨è¾¾å¼
            return self._skeletonize_regex(content)
    
    def _skeletonize_ast(self, content: str) -> str:
        """ä½¿ç”¨ AST è¿›è¡Œéª¨æ¶åŒ–"""
        tree = ast.parse(content)
        skeleton_lines = []
        
        # æ”¶é›†æ‰€æœ‰ import è¯­å¥
        imports = []
        for node in ast.walk(tree):
            if isinstance(node, (ast.Import, ast.ImportFrom)):
                imports.append(ast.get_source_segment(content, node))
        
        if imports:
            skeleton_lines.extend(imports)
            skeleton_lines.append("")
        
        # æ”¶é›†ç±»å’Œå‡½æ•°å®šä¹‰
        for node in ast.iter_child_nodes(tree):
            if isinstance(node, ast.ClassDef):
                skeleton_lines.append(self._extract_class_skeleton(node, content))
            elif isinstance(node, ast.FunctionDef):
                skeleton_lines.append(self._extract_function_signature(node, content))
        
        return "\n".join(skeleton_lines)
    
    def _extract_class_skeleton(self, node: ast.ClassDef, source: str) -> str:
        """æå–ç±»çš„éª¨æ¶"""
        lines = []
        
        # ç±»å®šä¹‰è¡Œ
        class_def = f"class {node.name}"
        if node.bases:
            bases = ", ".join(ast.get_source_segment(source, base) for base in node.bases)
            class_def += f"({bases})"
        class_def += ":"
        lines.append(class_def)
        
        # æ–‡æ¡£å­—ç¬¦ä¸²
        docstring = ast.get_docstring(node)
        if docstring:
            lines.append(f'    """{docstring}"""')
        
        # æ–¹æ³•ç­¾å
        for item in node.body:
            if isinstance(item, ast.FunctionDef):
                sig = self._extract_function_signature(item, source, indent="    ")
                lines.append(sig)
        
        if len(lines) == 1:
            lines.append("    pass")
        
        return "\n".join(lines)
    
    def _extract_function_signature(self, node: ast.FunctionDef, source: str, indent: str = "") -> str:
        """æå–å‡½æ•°ç­¾å"""
        lines = []
        
        # å‡½æ•°å®šä¹‰è¡Œ
        args_list = []
        for arg in node.args.args:
            args_list.append(arg.arg)
        
        func_def = f"{indent}def {node.name}({', '.join(args_list)}):"
        lines.append(func_def)
        
        # æ–‡æ¡£å­—ç¬¦ä¸²
        docstring = ast.get_docstring(node)
        if docstring:
            # åªä¿ç•™ç¬¬ä¸€è¡Œ
            first_line = docstring.split('\n')[0]
            lines.append(f'{indent}    """{first_line}"""')
        
        # æŠ˜å å®ç°
        lines.append(f"{indent}    # ... [Implementation]")
        
        return "\n".join(lines)
    
    def _skeletonize_regex(self, content: str) -> str:
        """ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼è¿›è¡Œéª¨æ¶åŒ– (é™çº§æ–¹æ¡ˆ)"""
        lines = content.split('\n')
        skeleton = []
        
        for line in lines:
            stripped = line.strip()
            
            # ä¿ç•™ import è¯­å¥
            if stripped.startswith(('import ', 'from ')):
                skeleton.append(line)
            
            # ä¿ç•™ç±»å®šä¹‰
            elif stripped.startswith('class '):
                skeleton.append(line)
            
            # ä¿ç•™å‡½æ•°å®šä¹‰
            elif stripped.startswith('def '):
                skeleton.append(line)
                # æ·»åŠ æŠ˜å æ ‡è®°
                indent = len(line) - len(line.lstrip())
                skeleton.append(' ' * (indent + 4) + '# ... [Implementation]')
            
            # ä¿ç•™æ–‡æ¡£å­—ç¬¦ä¸²çš„å¼€å§‹
            elif stripped.startswith('"""') or stripped.startswith("'''"):
                skeleton.append(line)
        
        return "\n".join(skeleton)
    
    def _truncate_priority_files(self, files: Dict[str, str], max_tokens: int) -> Dict[str, str]:
        """å½“ä¼˜å…ˆçº§æ–‡ä»¶è¶…é™æ—¶,è¿›è¡Œæˆªæ–­"""
        result = {}
        remaining_tokens = max_tokens
        
        for file, content in files.items():
            skeleton = self._skeletonize(content)
            file_text = self._format_file(file, skeleton)
            tokens = self.count_tokens(file_text)
            
            if tokens <= remaining_tokens:
                result[file] = skeleton
                remaining_tokens -= tokens
            else:
                print(f"âš ï¸ Skipping priority file {file} (not enough tokens)")
        
        return result
    
    def estimate_output_tokens(self, plan_content: str) -> int:
        """
        ä¼°ç®—è¾“å‡ºæ‰€éœ€çš„ tokens
        Estimate tokens needed for output
        
        åŸºäº PLAN.md çš„å¤æ‚åº¦ä¼°ç®—
        Based on PLAN.md complexity
        
        Args:
            plan_content: PLAN.md å†…å®¹
        
        Returns:
            ä¼°ç®—çš„è¾“å‡º token æ•°é‡
        """
        plan_tokens = self.count_tokens(plan_content)
        
        # ç»éªŒå…¬å¼: è¾“å‡º tokens â‰ˆ PLAN tokens * 3
        estimated = plan_tokens * 3
        
        # æœ€å°å€¼å’Œæœ€å¤§å€¼é™åˆ¶
        estimated = max(2048, min(estimated, 8192))
        
        print(f"ğŸ“Š Estimated output tokens: {estimated} (based on PLAN: {plan_tokens} tokens)")
        
        return estimated
    
    def get_token_stats(self, files: Dict[str, str]) -> Dict[str, int]:
        """
        è·å–æ–‡ä»¶çš„ Token ç»Ÿè®¡
        Get token statistics for files
        
        Returns:
            {file_path: token_count}
        """
        stats = {}
        for file, content in files.items():
            file_text = self._format_file(file, content)
            stats[file] = self.count_tokens(file_text)
        
        return stats


if __name__ == "__main__":
    # æµ‹è¯•ä¸Šä¸‹æ–‡ç®¡ç†å™¨
    print("ğŸ§ª Testing Context Manager...")
    
    manager = ContextManager(max_tokens=16384)
    
    # æµ‹è¯• Token è®¡æ•°
    test_code = """
def hello():
    print('world')
    
class MyClass:
    def __init__(self):
        pass
"""
    tokens = manager.count_tokens(test_code)
    print(f"\nğŸ“Š Test code tokens: {tokens}")
    
    # æµ‹è¯•éª¨æ¶åŒ–
    skeleton = manager._skeletonize(test_code)
    print(f"\nğŸ“‹ Skeleton:\n{skeleton}")
    
    skeleton_tokens = manager.count_tokens(skeleton)
    print(f"\nğŸ“Š Skeleton tokens: {skeleton_tokens} (saved {tokens - skeleton_tokens} tokens)")
    
    # æµ‹è¯•ä¸Šä¸‹æ–‡ä¼˜åŒ–
    files = {
        "main.py": test_code * 100,  # å¤§æ–‡ä»¶
        "utils.py": test_code * 50,
        "config.py": "CONFIG = {}"
    }
    
    optimized = manager.optimize_context(
        files, 
        priority_files=["main.py"],
        reserve_tokens=4096
    )
    
    print(f"\nâœ… Optimized: {len(optimized)}/{len(files)} files retained")
